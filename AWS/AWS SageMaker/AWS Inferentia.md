# AWS Inferentia

* <https://aws.amazon.com/machine-learning/neuron/>
* [Amazon EC2 Inf2 Instances](https://aws.amazon.com/ec2/instance-types/inf2/) - High performance at the lowest cost in Amazon EC2 for generative AI inference
* [AWS Neuron @ HF](https://huggingface.co/aws-neuron)

Articles:

* [Deploy large language models on AWS Inferentia2 using large model inference containers](https://aws.amazon.com/blogs/machine-learning/deploy-large-language-models-on-aws-inferentia2-using-large-model-inference-containers/)
* [Make your llama generation time fly with AWS Inferentia2](https://huggingface.co/blog/inferentia-llama2)
* [Deploy Llama 2 7B on AWS inferentia2 with Amazon SageMaker](https://www.philschmid.de/inferentia2-llama-7b) - learn how to deploy and speed up Llama 2 inference using AWS Inferentia2 and optimum-neuron on Amazon SageMaker.
* [Deploy Embedding Models on AWS inferentia2 with Amazon SageMaker](https://www.philschmid.de/inferentia2-embeddings) - learn how to deploy and speed up Embeddings Model inference using AWS Inferentia2 and optimum-neuron on Amazon SageMaker.
* [Fine-tune Falcon 180B with QLoRA and Flash Attention on Amazon SageMaker](https://www.philschmid.de/sagemaker-falcon-180b-qlora) - we are going to learn how to fine-tune [tiiuae/falcon-180B](https://huggingface.co/tiiuae/falcon-180B) using [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) with [Flash Attention](https://arxiv.org/abs/2205.14135.
* [Compiling HuggingFace models for AWS Inferentia with SageMaker Neo](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker_neo_compilation_jobs/deploy_huggingface_model_on_Inf1_instance/inf1_bert_compile_and_deploy.ipynb) - use SageMaker Neo to compile model for Inferentia instances.  Also [sagemaker-examples docs](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_neo_compilation_jobs/deploy_huggingface_model_on_Inf1_instance/inf1_bert_compile_and_deploy.html)