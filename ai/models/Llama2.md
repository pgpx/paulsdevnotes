# Llama2

Articles:

* [Deploy LLama2 13B Chat LMI Model with response streaming on SageMaker using Continuous Batching](https://github.com/windson/amazon-sagemaker-llama2-response-streaming-recipes/blob/main/llama-2-lmi/llama-2-13b-chat/1-deploy-llama-2-13b-chat-lmi-response-streaming.ipynb)