# * Falcon

* [tiiuae/falcon-180B-chat](https://huggingface.co/tiiuae/falcon-180B-chat)
    * <https://huggingface.co/blog/falcon>
    * [Falcon 180B foundation model from TII is now available via Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/falcon-180b-foundation-model-from-tii-is-now-available-via-amazon-sagemaker-jumpstart/) - needs `ml.p4de.24xlarge` instances - 8-bit quantized can run on `ml.p4d.24xlarge` but inference is 5x slower.
* [Deploy Falcon 180B on Amazon SageMaker](https://www.philschmid.de/sagemaker-falcon-180b)
* [Fine-tune Falcon 180B with QLoRA and Flash Attention on Amazon SageMaker](https://www.philschmid.de/sagemaker-falcon-180b-qlora)